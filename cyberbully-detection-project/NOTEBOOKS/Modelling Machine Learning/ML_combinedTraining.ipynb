{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4361d9c5",
   "metadata": {},
   "source": [
    "# INFORMASI\n",
    "### Metode Machine Learning\n",
    "### Ditrain dengan combined training setup (gabungan data YouTube dan Twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7389937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "addaf480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = pd.read_csv('../data/final_data_twitter.csv')\n",
    "df_yt = pd.read_csv('../data/final_data_yt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a07d9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter = df_twitter.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9307ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13005 entries, 0 to 13016\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   text             13005 non-null  object\n",
      " 1   cyberbullying    13005 non-null  int64 \n",
      " 2   length           13005 non-null  int64 \n",
      " 3   clean_text_bert  13005 non-null  object\n",
      " 4   clean_text_ML    13005 non-null  object\n",
      " 5   clean_text_ML_2  13005 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 711.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_twitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f8bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 650 entries, 0 to 649\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   text             650 non-null    object\n",
      " 1   cyberbullying    650 non-null    int64 \n",
      " 2   clean_text_bert  650 non-null    object\n",
      " 3   clean_text_ML    650 non-null    object\n",
      " 4   clean_text_ML_2  650 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 25.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_yt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e20cb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13655 entries, 0 to 13654\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   text             13655 non-null  object \n",
      " 1   cyberbullying    13655 non-null  int64  \n",
      " 2   length           13005 non-null  float64\n",
      " 3   clean_text_bert  13655 non-null  object \n",
      " 4   clean_text_ML    13655 non-null  object \n",
      " 5   clean_text_ML_2  13655 non-null  object \n",
      " 6   platform         13655 non-null  object \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 746.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_twitter['platform'] = 'twitter'\n",
    "df_yt['platform'] = 'youtube'\n",
    "\n",
    "df_all = pd.concat([df_twitter, df_yt], axis=0).reset_index(drop=True)\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f112a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_all['clean_text_ML_2']\n",
    "y = df_all['cyberbullying']\n",
    "\n",
    "# train + validation vs test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# split lagi train vs validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b00cd39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask Twitter dan YouTube berdasarkan original dataset\n",
    "mask_twitter = df_all['platform'] == 'twitter'\n",
    "mask_youtube = df_all['platform'] == 'youtube'\n",
    "\n",
    "X_test_twitter = X_test[mask_twitter.loc[X_test.index]]\n",
    "y_test_twitter = y_test[mask_twitter.loc[X_test.index]]\n",
    "\n",
    "X_test_youtube = X_test[mask_youtube.loc[X_test.index]]\n",
    "y_test_youtube = y_test[mask_youtube.loc[X_test.index]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc35f4b",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e8aeb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "def sentence_vector(text, embedding_model, dim=100):\n",
    "    words = text.split()\n",
    "    vectors = []\n",
    "    for w in words:\n",
    "        if w in embedding_model:\n",
    "            vectors.append(embedding_model[w])\n",
    "    \n",
    "    # Jika tidak ada satupun kata punya embedding -> return zero vector\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)\n",
    "    \n",
    "    # Rata-rata embedding\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8caef",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2e16151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.9,\n",
    "    min_df=5\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf   = tfidf.transform(X_val)\n",
    "X_test_tfidf  = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ad6a5",
   "metadata": {},
   "source": [
    "#### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9abe6a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "def load_glove(glove_file, dim=50):\n",
    "    glove_model = KeyedVectors(vector_size=dim)\n",
    "    \n",
    "    with open(glove_file, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            glove_model.add_vector(word, vector)\n",
    "    \n",
    "    return glove_model\n",
    "\n",
    "# Load model\n",
    "glove_vectors = load_glove(\"../model pretrained/glove/glove_50dim_wiki.id.case.text.txt\", dim=50)\n",
    "\n",
    "X_train_glove = np.vstack([sentence_vector(s, glove_vectors, dim=50) for s in X_train])\n",
    "X_val_glove   = np.vstack([sentence_vector(s, glove_vectors, dim=50) for s in X_val])\n",
    "X_test_glove  = np.vstack([sentence_vector(s, glove_vectors, dim=50) for s in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213ad05c",
   "metadata": {},
   "source": [
    "#### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2eaec1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext_model = gensim.models.fasttext.load_facebook_model(\"../model pretrained/fasttext/cc.id.300.bin\") # Dilatih dari data Common Crawl\n",
    "fasttext_vectors = fasttext_model.wv\n",
    "fasttext_dim = fasttext_vectors.vector_size\n",
    "\n",
    "\n",
    "X_train_fasttext = np.vstack([sentence_vector(text, fasttext_vectors, dim=fasttext_dim) for text in X_train])\n",
    "X_val_fasttext = np.vstack([sentence_vector(text, fasttext_vectors, dim=fasttext_dim) for text in X_val])\n",
    "X_test_fasttext = np.vstack([sentence_vector(text, fasttext_vectors, dim=fasttext_dim) for text in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa6f8a",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cf0b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_wiki = Word2Vec.load(\"../model pretrained/word2vec/idwiki_word2vec_100/idwiki_word2vec_100.model\")\n",
    "w2v_vectors = w2v_wiki.wv\n",
    "w2v_dim = w2v_vectors.vector_size\n",
    "\n",
    "X_train_w2v = np.vstack([sentence_vector(text, w2v_vectors, dim=w2v_dim) for text in X_train])\n",
    "X_val_w2v = np.vstack([sentence_vector(text, w2v_vectors, dim=w2v_dim) for text in X_val])\n",
    "X_test_w2v = np.vstack([sentence_vector(text, w2v_vectors, dim=w2v_dim) for text in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7887a0",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560802d5",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a70eb05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:44:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:44:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:44:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:44:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Text Representation</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.812283</td>\n",
       "      <td>0.762786</td>\n",
       "      <td>0.819962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.814645</td>\n",
       "      <td>0.841755</td>\n",
       "      <td>0.688792</td>\n",
       "      <td>0.803787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>FastText CC (Common Crawl)</td>\n",
       "      <td>0.767506</td>\n",
       "      <td>0.748489</td>\n",
       "      <td>0.673558</td>\n",
       "      <td>0.757726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>FastText CC (Common Crawl)</td>\n",
       "      <td>0.762929</td>\n",
       "      <td>0.764861</td>\n",
       "      <td>0.630033</td>\n",
       "      <td>0.749326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>0.700229</td>\n",
       "      <td>0.658654</td>\n",
       "      <td>0.596300</td>\n",
       "      <td>0.687916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>0.687414</td>\n",
       "      <td>0.650510</td>\n",
       "      <td>0.554951</td>\n",
       "      <td>0.671425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Glove Wikipedia</td>\n",
       "      <td>0.681007</td>\n",
       "      <td>0.632143</td>\n",
       "      <td>0.577802</td>\n",
       "      <td>0.668402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.639817</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.897715</td>\n",
       "      <td>0.634961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Glove Wikipedia</td>\n",
       "      <td>0.657208</td>\n",
       "      <td>0.625740</td>\n",
       "      <td>0.460283</td>\n",
       "      <td>0.630249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Glove Wikipedia</td>\n",
       "      <td>0.623799</td>\n",
       "      <td>0.546147</td>\n",
       "      <td>0.624592</td>\n",
       "      <td>0.620121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>0.592677</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>0.731230</td>\n",
       "      <td>0.592472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>FastText CC (Common Crawl)</td>\n",
       "      <td>0.530892</td>\n",
       "      <td>0.465806</td>\n",
       "      <td>0.785637</td>\n",
       "      <td>0.522831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model         Text Representation  Accuracy  Precision    Recall  \\\n",
       "0           SVM                      TF-IDF  0.826087   0.812283  0.762786   \n",
       "1       XGBoost                      TF-IDF  0.814645   0.841755  0.688792   \n",
       "2       XGBoost  FastText CC (Common Crawl)  0.767506   0.748489  0.673558   \n",
       "3           SVM  FastText CC (Common Crawl)  0.762929   0.764861  0.630033   \n",
       "4       XGBoost          Word2Vec Wikipedia  0.700229   0.658654  0.596300   \n",
       "5           SVM          Word2Vec Wikipedia  0.687414   0.650510  0.554951   \n",
       "6       XGBoost             Glove Wikipedia  0.681007   0.632143  0.577802   \n",
       "7   Naive Bayes                      TF-IDF  0.639817   0.543478  0.897715   \n",
       "8           SVM             Glove Wikipedia  0.657208   0.625740  0.460283   \n",
       "9   Naive Bayes             Glove Wikipedia  0.623799   0.546147  0.624592   \n",
       "10  Naive Bayes          Word2Vec Wikipedia  0.592677   0.511027  0.731230   \n",
       "11  Naive Bayes  FastText CC (Common Crawl)  0.530892   0.465806  0.785637   \n",
       "\n",
       "    F1 Macro  \n",
       "0   0.819962  \n",
       "1   0.803787  \n",
       "2   0.757726  \n",
       "3   0.749326  \n",
       "4   0.687916  \n",
       "5   0.671425  \n",
       "6   0.668402  \n",
       "7   0.634961  \n",
       "8   0.630249  \n",
       "9   0.620121  \n",
       "10  0.592472  \n",
       "11  0.522831  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Representasi teks\n",
    "representations = {\n",
    "    'TF-IDF': (X_train_tfidf.toarray(), X_val_tfidf.toarray()),\n",
    "    'Glove Wikipedia': (X_train_glove, X_val_glove),\n",
    "    'FastText CC (Common Crawl)': (X_train_fasttext, X_val_fasttext),\n",
    "    'Word2Vec Wikipedia': (X_train_w2v, X_val_w2v)  \n",
    "}\n",
    "\n",
    "# Models\n",
    "models = {\n",
    "    'SVM': SVC(kernel='linear', random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='auc', random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for rep_name, (X_tr, X_vl) in representations.items():\n",
    "        model.fit(X_tr, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_vl)\n",
    "        \n",
    "        # Metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        prec = precision_score(y_val, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_val, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "        \n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Text Representation': rep_name,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1 Macro': f1\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='F1 Macro', ascending=False)\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce4038",
   "metadata": {},
   "source": [
    "#### Handling Imbalanced (Balanced Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af5d79ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:46:06] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:46:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:46:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:46:09] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Text Representation</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.827002</td>\n",
       "      <td>0.785037</td>\n",
       "      <td>0.810664</td>\n",
       "      <td>0.823283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.810526</td>\n",
       "      <td>0.800954</td>\n",
       "      <td>0.731230</td>\n",
       "      <td>0.803003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>FastText CC (Common Crawl)</td>\n",
       "      <td>0.767048</td>\n",
       "      <td>0.730856</td>\n",
       "      <td>0.706202</td>\n",
       "      <td>0.759861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>FastText CC (Common Crawl)</td>\n",
       "      <td>0.752403</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>0.723613</td>\n",
       "      <td>0.747182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>0.696568</td>\n",
       "      <td>0.644796</td>\n",
       "      <td>0.620239</td>\n",
       "      <td>0.687001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>0.678261</td>\n",
       "      <td>0.602857</td>\n",
       "      <td>0.688792</td>\n",
       "      <td>0.675086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Glove Wikipedia</td>\n",
       "      <td>0.682838</td>\n",
       "      <td>0.623904</td>\n",
       "      <td>0.619151</td>\n",
       "      <td>0.674288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Glove Wikipedia</td>\n",
       "      <td>0.645767</td>\n",
       "      <td>0.568851</td>\n",
       "      <td>0.651795</td>\n",
       "      <td>0.642368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.639817</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.897715</td>\n",
       "      <td>0.634961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Glove Wikipedia</td>\n",
       "      <td>0.623799</td>\n",
       "      <td>0.546147</td>\n",
       "      <td>0.624592</td>\n",
       "      <td>0.620121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>0.592677</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>0.731230</td>\n",
       "      <td>0.592472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>FastText CC (Common Crawl)</td>\n",
       "      <td>0.530892</td>\n",
       "      <td>0.465806</td>\n",
       "      <td>0.785637</td>\n",
       "      <td>0.522831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model         Text Representation  Accuracy  Precision    Recall  \\\n",
       "0           SVM                      TF-IDF  0.827002   0.785037  0.810664   \n",
       "1       XGBoost                      TF-IDF  0.810526   0.800954  0.731230   \n",
       "2       XGBoost  FastText CC (Common Crawl)  0.767048   0.730856  0.706202   \n",
       "3           SVM  FastText CC (Common Crawl)  0.752403   0.698529  0.723613   \n",
       "4       XGBoost          Word2Vec Wikipedia  0.696568   0.644796  0.620239   \n",
       "5           SVM          Word2Vec Wikipedia  0.678261   0.602857  0.688792   \n",
       "6       XGBoost             Glove Wikipedia  0.682838   0.623904  0.619151   \n",
       "7           SVM             Glove Wikipedia  0.645767   0.568851  0.651795   \n",
       "8   Naive Bayes                      TF-IDF  0.639817   0.543478  0.897715   \n",
       "9   Naive Bayes             Glove Wikipedia  0.623799   0.546147  0.624592   \n",
       "10  Naive Bayes          Word2Vec Wikipedia  0.592677   0.511027  0.731230   \n",
       "11  Naive Bayes  FastText CC (Common Crawl)  0.530892   0.465806  0.785637   \n",
       "\n",
       "    F1 Macro  \n",
       "0   0.823283  \n",
       "1   0.803003  \n",
       "2   0.759861  \n",
       "3   0.747182  \n",
       "4   0.687001  \n",
       "5   0.675086  \n",
       "6   0.674288  \n",
       "7   0.642368  \n",
       "8   0.634961  \n",
       "9   0.620121  \n",
       "10  0.592472  \n",
       "11  0.522831  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "models_balanced = {\n",
    "    'SVM': SVC(kernel='linear', class_weight='balanced', random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "        scale_pos_weight=pos_weight,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "results_class_weight = []\n",
    "\n",
    "for model_name, model in models_balanced.items():\n",
    "    for rep_name, (X_tr, X_vl) in representations.items():\n",
    "        \n",
    "        model.fit(X_tr, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_vl)\n",
    "        \n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        prec = precision_score(y_val, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_val, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred, average='macro', zero_division=0)\n",
    "        \n",
    "        results_class_weight.append({\n",
    "            'Model': model_name,\n",
    "            'Text Representation': rep_name,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1 Macro': f1\n",
    "        })\n",
    "\n",
    "results_class_weight_df = pd.DataFrame(results_class_weight)\n",
    "results_class_weight_df = results_class_weight_df.sort_values(by='F1 Macro', ascending=False)\n",
    "results_class_weight_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results_class_weight_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e38599",
   "metadata": {},
   "source": [
    "# Testing dan Hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3454b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Mapping posisi test\n",
    "test_idx = X_test.index\n",
    "test_pos_map = {idx: pos for pos, idx in enumerate(test_idx)}\n",
    "\n",
    "# 2. Posisi untuk twitter & youtube\n",
    "twitter_positions = [test_pos_map[i] for i in X_test_twitter.index]\n",
    "youtube_positions = [test_pos_map[i] for i in X_test_youtube.index]\n",
    "\n",
    "# 3. Text representations untuk test per platform\n",
    "representations_test_twitter = {\n",
    "    'TF-IDF': X_test_tfidf.toarray()[twitter_positions],\n",
    "    'Glove Wikipedia': X_test_glove[twitter_positions],\n",
    "    'FastText CC (Common Crawl)': X_test_fasttext[twitter_positions],\n",
    "    'Word2Vec Wikipedia': X_test_w2v[twitter_positions]\n",
    "}\n",
    "\n",
    "representations_test_youtube = {\n",
    "    'TF-IDF': X_test_tfidf.toarray()[youtube_positions],\n",
    "    'Glove Wikipedia': X_test_glove[youtube_positions],\n",
    "    'FastText CC (Common Crawl)': X_test_fasttext[youtube_positions],\n",
    "    'Word2Vec Wikipedia': X_test_w2v[youtube_positions]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47b9a054",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:48:40] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:48:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:48:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/Users/jowillg/anaconda3/envs/pytorch_DL/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [15:48:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Twitter_Accuracy</th>\n",
       "      <th>Twitter_Precision</th>\n",
       "      <th>Twitter_Recall</th>\n",
       "      <th>Twitter_F1</th>\n",
       "      <th>YouTube_Accuracy</th>\n",
       "      <th>YouTube_Precision</th>\n",
       "      <th>YouTube_Recall</th>\n",
       "      <th>YouTube_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.820414</td>\n",
       "      <td>0.775758</td>\n",
       "      <td>0.810860</td>\n",
       "      <td>0.817192</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.618902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>FastText CC (Common Crawl)</td>\n",
       "      <td>0.770146</td>\n",
       "      <td>0.716239</td>\n",
       "      <td>0.758371</td>\n",
       "      <td>0.766377</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.575792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>FastText CC (Common Crawl)</td>\n",
       "      <td>0.772448</td>\n",
       "      <td>0.738361</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.766170</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>0.694167</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.736652</td>\n",
       "      <td>0.692685</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.414634</td>\n",
       "      <td>0.386364</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Glove Wikipedia</td>\n",
       "      <td>0.698005</td>\n",
       "      <td>0.642729</td>\n",
       "      <td>0.647964</td>\n",
       "      <td>0.691194</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.645050</td>\n",
       "      <td>0.550056</td>\n",
       "      <td>0.895023</td>\n",
       "      <td>0.640379</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.527516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>0.706447</td>\n",
       "      <td>0.653153</td>\n",
       "      <td>0.656109</td>\n",
       "      <td>0.699686</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.510017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.816961</td>\n",
       "      <td>0.809055</td>\n",
       "      <td>0.743891</td>\n",
       "      <td>0.810394</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.503968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Glove Wikipedia</td>\n",
       "      <td>0.661550</td>\n",
       "      <td>0.585441</td>\n",
       "      <td>0.691403</td>\n",
       "      <td>0.659625</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.496869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Glove Wikipedia</td>\n",
       "      <td>0.654643</td>\n",
       "      <td>0.580266</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.652073</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.473722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>FastText CC (Common Crawl)</td>\n",
       "      <td>0.534919</td>\n",
       "      <td>0.471497</td>\n",
       "      <td>0.800905</td>\n",
       "      <td>0.525032</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.341772</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>0.447859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Word2Vec Wikipedia</td>\n",
       "      <td>0.606677</td>\n",
       "      <td>0.525221</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.606258</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.437082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model              Representation  Twitter_Accuracy  \\\n",
       "0           SVM                      TF-IDF          0.820414   \n",
       "1           SVM  FastText CC (Common Crawl)          0.770146   \n",
       "2       XGBoost  FastText CC (Common Crawl)          0.772448   \n",
       "3           SVM          Word2Vec Wikipedia          0.694167   \n",
       "4       XGBoost             Glove Wikipedia          0.698005   \n",
       "5   Naive Bayes                      TF-IDF          0.645050   \n",
       "6       XGBoost          Word2Vec Wikipedia          0.706447   \n",
       "7       XGBoost                      TF-IDF          0.816961   \n",
       "8           SVM             Glove Wikipedia          0.661550   \n",
       "9   Naive Bayes             Glove Wikipedia          0.654643   \n",
       "10  Naive Bayes  FastText CC (Common Crawl)          0.534919   \n",
       "11  Naive Bayes          Word2Vec Wikipedia          0.606677   \n",
       "\n",
       "    Twitter_Precision  Twitter_Recall  Twitter_F1  YouTube_Accuracy  \\\n",
       "0            0.775758        0.810860    0.817192             0.664   \n",
       "1            0.716239        0.758371    0.766377             0.664   \n",
       "2            0.738361        0.717647    0.766170             0.640   \n",
       "3            0.616667        0.736652    0.692685             0.592   \n",
       "4            0.642729        0.647964    0.691194             0.608   \n",
       "5            0.550056        0.895023    0.640379             0.528   \n",
       "6            0.653153        0.656109    0.699686             0.568   \n",
       "7            0.809055        0.743891    0.810394             0.600   \n",
       "8            0.585441        0.691403    0.659625             0.568   \n",
       "9            0.580266        0.670588    0.652073             0.536   \n",
       "10           0.471497        0.800905    0.525032             0.448   \n",
       "11           0.525221        0.753846    0.606258             0.440   \n",
       "\n",
       "    YouTube_Precision  YouTube_Recall  YouTube_F1  \n",
       "0            0.526316        0.454545    0.618902  \n",
       "1            0.541667        0.295455    0.575792  \n",
       "2            0.483871        0.340909    0.571429  \n",
       "3            0.414634        0.386364    0.545455  \n",
       "4            0.424242        0.318182    0.540200  \n",
       "5            0.411765        0.795455    0.527516  \n",
       "6            0.368421        0.318182    0.510017  \n",
       "7            0.384615        0.227273    0.503968  \n",
       "8            0.352941        0.272727    0.496869  \n",
       "9            0.315789        0.272727    0.473722  \n",
       "10           0.341772        0.613636    0.447859  \n",
       "11           0.319444        0.522727    0.437082  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_final = []\n",
    "\n",
    "for model_name, model in models_balanced.items():\n",
    "    for rep_name, (X_tr, X_vl) in representations.items():\n",
    "        \n",
    "        # TRAIN ON FULL TRAINING SET (BALANCED)\n",
    "        model.fit(X_tr, y_train)\n",
    "        \n",
    "        # TEST TWITTER \n",
    "        X_test_tw = representations_test_twitter[rep_name]\n",
    "        y_pred_tw = model.predict(X_test_tw)\n",
    "\n",
    "        acc_tw  = accuracy_score(y_test_twitter, y_pred_tw)\n",
    "        prec_tw = precision_score(y_test_twitter, y_pred_tw, zero_division=0)\n",
    "        rec_tw  = recall_score(y_test_twitter, y_pred_tw, zero_division=0)\n",
    "        f1_tw   = f1_score(y_test_twitter, y_pred_tw, average='macro', zero_division=0)\n",
    "        \n",
    "        \n",
    "        # TEST YOUTUBE \n",
    "        X_test_yt = representations_test_youtube[rep_name]\n",
    "        y_pred_yt = model.predict(X_test_yt)\n",
    "\n",
    "        acc_yt  = accuracy_score(y_test_youtube, y_pred_yt)\n",
    "        prec_yt = precision_score(y_test_youtube, y_pred_yt, zero_division=0)\n",
    "        rec_yt  = recall_score(y_test_youtube, y_pred_yt, zero_division=0)\n",
    "        f1_yt   = f1_score(y_test_youtube, y_pred_yt, average='macro', zero_division=0)\n",
    "        \n",
    "        \n",
    "        results_final.append({\n",
    "            'Model': model_name,\n",
    "            'Representation': rep_name,\n",
    "\n",
    "            # Twitter test\n",
    "            'Twitter_Accuracy': acc_tw,\n",
    "            'Twitter_Precision': prec_tw,\n",
    "            'Twitter_Recall': rec_tw,\n",
    "            'Twitter_F1': f1_tw,\n",
    "\n",
    "            # YouTube test\n",
    "            'YouTube_Accuracy': acc_yt,\n",
    "            'YouTube_Precision': prec_yt,\n",
    "            'YouTube_Recall': rec_yt,\n",
    "            'YouTube_F1': f1_yt\n",
    "        })\n",
    "\n",
    "results_final_df = pd.DataFrame(results_final)\n",
    "results_final_df = results_final_df.sort_values(by='YouTube_F1', ascending=False)\n",
    "results_final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "results_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e8c5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
